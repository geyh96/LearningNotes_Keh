
\chapter{Cube Root Asymptotics}
\section*{Overview}
This paper gives a functional central limit theory for empirical process.

\begin{itemize}
  \item many convergence rate $n^{-1/3}$
  \item Key point is: continuous mapping theorem for the location of maximum point.
  \item 
\end{itemize}

\marginpar{
  \begin{marginnotes}
    The paper is a very nice material for understanding the core idea and techniques of empirical process.

    Main reference of this paper is in the lecture notes of Pollard ``Empirical process: Theory and applications'' 1990 version.
  \end{marginnotes}
}





\subsection{The Mode Estimation Problem}
% \lipsum[2]

The paper first intuitively gives an example of ``mode estimation'' and gives its convergence rate $n^{-1/3}$

Suppose $\hat \theta_n$ is chosen to maximize

\begin{align}
    \Gamma_n(\theta) = P_n[\theta - 1,\theta + 1]
\end{align}
is the proportion of observations in an interval of length 2.

If $P$ has a smooth density $p(\dot)$,
the function $\Gamma$ is approximately parabolic of its optimal value $\theta_0$, which means that 
$$
\Gamma(\theta)-\Gamma\left(\theta_0\right)=\int_{1+\theta_0}^{1+\theta} p(x) d x-\int_{-1+\theta_0}^{-1+\theta} p(x) d x \approx-C \left(\theta-\theta_0\right)^2
$$

\begin{summary}
  Take care that $\Gamma(\theta)$ is the expectation of $\Gamma_n(\theta)$.
  The term above  is the ``bias'' caused by the departure of $\theta$ from $\theta_0$.
\end{summary}

Then we consider the stochastic term,
$$
D_n(\theta)=\left[\Gamma_n(\theta)-\Gamma_n\left(\theta_0\right)\right]-\left[\Gamma(\theta)-\Gamma\left(\theta_0\right)\right] .
$$

For fixed $\theta$,
the $D_n(\theta)$ is approximately $N(0,\sigma^2_\theta/n)$
where
$$
\sigma_\theta^2 \approx \int_{1+\theta_0}^{1+\theta} p(x) d x+\int_{-1+\theta_0}^{-1+\theta} p(x) d x \approx C\left|\theta-\theta_0\right|
$$
\marginpar{
  \begin{marginnotes}
The ``add'' and ``minus'' produces different order here.
The order produced by ``add'' is due to the finite density of  $p()$.
Thus the density integral is proportional to the length of $\theta$
  \end{marginnotes}
}
% \lipsum[2]
Intuitively, when the bias term $C|\theta - \theta_0|^2$  is large comparing with the stochastic term $C|\theta - \theta_0|$ , the $\theta$ is far away from the true value $\theta_0$.
Thus not maximize the $\Gamma_n(\theta)$.

So the $\theta$ could be the solution of $\Gamma_n(\theta)$ if the bias term is the same order or smaller than the stochastic term.
It means 
\begin{align*}
  C|\theta - \theta_0|^2 &< C n^{-1/2}|\theta - \theta_0|^{1/2} \\
  C|\theta - \theta_0|^{3/2} &< C n^{-1/2} \\
  C|\theta - \theta_0| &< C n^{-1/3} \\
\end{align*}

\begin{summary}
  However, it is just an intuitive explaination.
  theoretically, we need build error bound uniformly in $\theta$ and the normal approximation must hold uniformly over $\theta$.
\end{summary}

Note that the variance term $\sigma_\theta$ decreases with $|\theta - \theta_0|$.

If the loss function $g(\theta,\dot)$ is differentiable, 
$\sigma_\theta$ decreases with $|\theta - \theta_0|^2$.
 
Thus
\begin{align*}
  C|\theta - \theta_0|^2 &< C n^{-1/2}|\theta - \theta_0| \\
  C|\theta - \theta_0| &< C n^{-1/2}
\end{align*}

It produces the common $n^{-1/2}$ rate.

Thus the  variance term $\sigma_\theta$ decreases with $|\theta - \theta_0|$, the non-standard case is a consequence of ``shape-edge effect''


\subsection{Convergence in distribution and the argmax functional}