
\chapter{ROBUST NONPARAMETRIC REGRESSION WITH DEEP NEURAL NETWORKS}
\section*{Overview}

\marginpar{
  \begin{marginnotes}
    ROBUST NONPARAMETRIC REGRESSION WITH DEEP NEURAL NETWORKS
  \end{marginnotes}
}
Use Deep neural network do regression.
Main contributions lies in the theoretical part.
\begin{itemize}
  \item excess risk grows with d, the dimension of data in sublinear form
  \item only require that the Y has finite p order moment. it is the key point of the ROBUST in the title.
  \item loose the assumption of exact manifold suppurt assumption
  \item limitation: X have bounded support. The condition is needed in the approximation theory.
  The condition appears in page 10. (Writting skills.)
\end{itemize}

$f^0$ is the true function we want.
$Z= (X,Y)$ is random vector independent of $f$.  \Kchange{So no post selection problem here.? why emphisize it.}
$Y =  f^0(X) + \eta$, where $\eta$ is the noise vector independent with $X$.
$L$ is the loss function that is Lipschitz and continuous.


So we can define the risk we interested in :
$$
R(f) = E_z L\{f(x),Y\}
$$

and we define the target here is 
$f^* = \argmin_f R(f) = \argmin_f E_z L\{f(x),Y\}$

\Kchange{The target of optimization and what we optimize is in the same page with the definition of $f^*$}.

We denote $\mS = \{X_i,Y_i\}_{i=1}^n$ is the data set with sample size $n$.

Then we can define the empirical risk on the data set is :
$$
R_n(f) = \frac{1}{n} \sum_i L\{f(X_i),Y_i\}
$$
.

We define the estimated function as
$$
\hat f_n = \argmin\limits_{f\in \mF_n} R_n(f)
$$
The estimated function must be defined in a proper function class $\mF_n$. The subscripts $n$ says the depedency of the function class with the sample size $n$.

So we have the excess risk:
$$
R(\hat f_n) - R(f^*) = E_z L\{\hat f_n(X),Y\} - E_z L\{f^*(X),Y\}
$$
\Kchange{The excess risk is the key point in machine learning
We think a ``better'' $f_n$ should have smaller excess risk.
However, the excess risk is still unobeservable.
}

And recall that $\hat f_n$ depends on the data, so the definition above is actually random.
We can also investigate its expectation, $E_z\{R(\hat f_n) - R(f^*)\}$.

Then we describe the function class $\mF_n$, 
an important character in this work is it distinguishes the different effect of the model depth $\mD$ and the width $\mW$.

\section{Basic Error Analysis}
For any $f \in \mF_\phi$,
\begin{align}
  R(f) - R(f^*) = \{R(f) - \inf_{f\in \mF_\phi}\} + \{\inf_{f\in \mF_\phi} - R(f^*)\} 
\end{align}

The first guy in the RHS is stochastic error.
The second guy in the RHS is approximation error.

consider a data-dependent $f_n$.
According to Jiao 2021,
\begin{align}
  R(\hat f_n) - R(f^*) \leq 2\sup_{f\in \mF_n} |R(f) - R_n(f)| + \inf_{f\in\mF_n} R(f) - R(f^*) 
\end{align}

The first term can be handled with empirical process.
\Kchange{When model space enlarged, this guy will increase.}
The second term can be handled with approximation theory.
\Kchange{When model spcae enlarged, this guy will decrease.}
The approximation error part has four reference I saved in mendeley.

\Kchange{Intuitively, Larger model space can appromixate model better but tend to overfitting.}

In the approximation theory part,
the key point is bound $ \inf_{f\in\mF_n} R(f) - R(f^*) $ with the term $\in_{f\in\mF_n} |f- f^*|$ where $|\dot|$ is the metric in the appropriate space.
\Kchange{it depdends on the modulus of continuity of the function}.

\subsection{Stochastic Error}
Key definition, \textbf{Pseudo Dimension} \Kchange{I have heard this guy.}
,For function class $\mF: \mX\to \bR$, we denote its Pseudo dimension as $Pdim(\mF)$.

\Kchange{I donot know the difference of the definition of Pdim and Shatter number.}

However, for the neural network class, we have $Pdim(\mF) = VC(\mF)$.

Need to verify we investigate the Pdim and covering number of the point set given $f$ and dataset of size $n$.
Then the uniform covering number is make $sup$ to the empirical covering number.

Consider Lemma3,
\Kchange{the condition $Pdim(\mF)<2n$, I cannot get the intuition.}

reference Bartlett 2019.

\Kchange{Indeed, it is a high dimensional result with parameter sparsity.}

\section{Approximation Error}

We approximate $f^*$ with $\mF_\phi$, the function class of neural network.

\Kchange{X distributed on a bounded support.}




